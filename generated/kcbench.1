.\" Automatically generated by Pandoc 2.9.2.1
.\"
.TH "kcbench" "1" "" "Version 0.9" "User Commands"
.hy
.SH NAME
.PP
kcbench - Linux kernel compile benchmark, speed edition
.SH SYNOPSIS
.PP
\f[B]kcbench\f[R] [\f[B]options\f[R]]
.SH DESCRIPTION
.PP
Kcbench tries to compile a Linux kernel really quickly which can be used
to test a system\[aq]s performance or stability.
.PP
Note: The number of compile jobs (\[aq]-j\[aq]) that delivers the best
result depends on the machine being benched.
See the section \[dq]ON THE DEFAULT NUMBER OF JOBS\[dq] below for
details.
.PP
To get comparable results from different machines you need to use the
exact same operating system on all of them.
There are multiple reasons for this recommendation, but one of the main
reasons is: the Linux version this benchmark downloads and compiles
depends on the operating system\[aq]s default compiler.
.PP
If you choose to ignore this recommendation at least make sure to hard
code the Linux version to compile (\[aq]-s 5.4\[aq]), as for example
compiling 5.7 will take longer than 5.4 or 4.19 and thus lead to results
one cannot compare.
Also, make sure the compiler used on the systems you want to compare is
from similar, as for example gcc10 will try harder to optimize the code
than gcc8 or gcc9 and thus take more time for its work.
.PP
Kcbench is accompanied by kcbenchrate.
Both are quite similar, but work slightly different:
.IP \[bu] 2
kcbench tries to build one kernel as fast as possible.
This approach is called \[aq]speed run\[aq] and let\[aq]s make start
multiple compilers jobs in parallel by using \[aq]make -j #\[aq].
That way kcbench will use a lot of CPU cores most of the time, except
during those few phases where the Linux kernel build process is singled
threaded and thus utilizes just one CPU core.
That for example is the case when vmlinux is linked.
.IP \[bu] 2
kcbenchrate tries to keep all CPU cores busy constantly by starting
workers on all of them, which each builds one kernel with just one job
(\[aq]make -j 1\[aq]).
This approach is called \[aq]rate run\[aq].
It takes a lot longer to generate a result than kcbench; it also needs a
lot more storage space, but will utilize the machine and its processors
better.
.SS Options
.TP
\f[B]-b\f[R], \f[B]--bypass\f[R]
Omit the initial kernel compile to fill caches; saves time, but first
result might be slightly lower than the following ones.
.TP
\f[B]-d\f[R], \f[B]--detailedresults\f[R]
Print more detailed results.
.TP
\f[B]-h\f[R], \f[B]--help\f[R]
Show usage.
.TP
\f[B]-i\f[R], \f[B]--iterations\f[R] \f[I]int\f[R]
Determines the number of kernels that kcbench will compile sequentially
with different values of jobs (\[aq]-j\[aq]).
Default: 2
.TP
\f[B]-j\f[R], \f[B]--jobs\f[R] \f[I]int\f[R](,\f[I]int\f[R], \f[I]int\f[R], ...)
Number of jobs to use when compiling a kernel(\[aq]make -j #\[aq]).
.RS
.PP
This option can be given multiple times (-j 2 -j 4 -j 8) or
\[aq]\f[I]int\f[R]\[aq] can be a list (-j \[dq]2 4 8\[dq]).
The default depends on the number of cores in the system and if its
processor uses SMT.
Run \[aq]--help\[aq] to query the default on the particular machine.
.PP
Important note: kcbench on machines with SMTs will do runs which do not
utilize all available CPU cores; this might look odd, but there are
reasons for this behaviour.
See \[dq]ON THE DEFAULT NUMBER OF JOBS\[dq] below for details.
.RE
.TP
\f[B]-m\f[R], \f[B]--modconfig\f[R]
Instead of using a config generated with \[aq]defconfig\[aq] use one
built by \[aq]allmodconfig\[aq] and compile modules as well.
Takes a lot longer to compile, which is more suitable for machines with
a lot of fast CPU cores.
.TP
\f[B]-o\f[R], \f[B]--outputdir\f[R] \f[I]dir\f[R]
Use \f[I]path\f[R] to compile Linux.
Passes \[aq]O=\f[I]dir\f[R]/kcbench-worker/\[aq] to make when calling it
to compile a kernel; use a temporary directory if not given.
.TP
\f[B]-s\f[R], \f[B]--src\f[R] \f[I]path\f[R]|\f[I]version\f[R]
Look for sources in \f[I]path\f[R],
\[ti]/.cache/kcbench/linux-\f[I]version\f[R] or
/usr/share/kcbench/linux-\f[I]version\f[R].
If not found try to download \f[I]version\f[R] automatically unless
\[aq]--no-download\[aq] was specified.
.TP
\f[B]-v\f[R], \f[B]--verbose\f[R]
Increase verboselevel; option can be given multiple times.
.TP
\f[B]-V\f[R], \f[B]--version\f[R]
Output program version.
.TP
\f[B]--cc \f[BI]exec\f[B]\f[R]
Use \f[I]exec\f[R] as target compiler.
.TP
\f[B]--cross-compile \f[BI]arch\f[B]\f[R]
EXPERIMENTAL: Cross compile the Linux kernel.
Cross compilers for this task are packaged in some Linux distribution.
There are also pre-compiled compilers available on the internet, for
example here: https://mirrors.edge.kernel.org/pub/tools/crosstool/
.RS
.PP
Values of \f[I]arch\f[R] that kcbench/kcbenchrate understand: arm arm64
aarch64 riscv riscv64 powerpc powerpc64 x86_64
.PP
Building for archs not directly supported by kcbench/kcbenchrate should
work, too: just export ARCH= and CROSS_COMPILE= just like you would when
normally cross compiling a Linux kernel.
Do not use \[aq]--cross-compile\[aq] in that case and keep in mind that
kcbench/kcbenchrate configure the compiled Linux kernel with the make
target \[aq]defconfig\[aq] (or \[aq]allmodconfig\[aq], if you specify
\[aq]-m\[aq]), which might be unusual for the arch in question, but
might be good enough for benchmarking purposes.
.PP
Be aware there is a bigger risk running into compile errors (see below)
when cross compiling.
.RE
.TP
\f[B]--crosscomp-scheme \f[BI]scheme\f[B]\f[R]
On Linux distributions that are known to ship cross compilers kcbench/
kcbenchrate will assume you want to use those.
This parameter allows to specify one of the various different naming
schemes in cases this automatic detection fails or work you want
kcbench/kcbenchrate to find them using a \[aq]generic\[aq] scheme that
should work with compilers from various sources, which is the default on
unknown distributions.
.RS
.PP
Valid values of \f[I]scheme\f[R]: debian fedora generic redhat ubuntu
.RE
.TP
\f[B]--hostcc \f[BI]exec\f[B]\f[R]
Use \f[I]exec\f[R] as host compiler.
.TP
\f[B]--infinite\f[R]
Run endlessly to create system load.
.TP
\f[B]--llvm\f[R]
Set LLVM=1 to use clang as compiler and LLVM utilities as GNU binutils
substitute.
.TP
\f[B]--add-make-args \f[BI]string\f[B]\f[R]
Pass additional flags found in \f[I]string\f[R] to \f[C]make\f[R] when
creating the config or building the kernel.
This option is meant for experts that want to try unusual things, like
specifying a special linker
(\f[C]--add-make-args \[aq]LD=ld.lld\[aq]\f[R]).
.RS
.PP
Use with caution!
.RE
.TP
\f[B]--no-download\f[R]
Never download Linux kernel sources from the web automatically.
.TP
\f[B]--savefailedlogs \f[BI]path\f[B]\f[R]
Save log of failed compile runs to \f[I]path\f[R].
.SH ON THE DEFAULT NUMBER OF JOBS
.PP
The optimal number of compile jobs (-j) to get the best result depends
on the machine being benched.
On most systems you will achieve the best result if the number of jobs
matches the number of CPU cores.
That for example is the case on this 4 core Intel processor without SMT:
.IP
.nf
\f[C]
[cttest\[at]localhost \[ti]]$ bash kcbench -s 5.3 -n 1
Processor:            Intel(R) Core(TM) i5-4570 CPU \[at] 3.20GHz [4 CPUs]
Cpufreq; Memory:      Unknown; 15934 MByte RAM
Compiler used:        gcc (GCC) 9.2.1 20190827 (Red Hat 9.2.1-1)
Linux compiled:       5.3.0 [/home/cttest/.cache/kcbench/linux-5.3/]
Config; Environment:  defconfig; CCACHE_DISABLE=\[dq]1\[dq]
Build command:        make vmlinux
Run 1 (-j 4):         250.03 seconds / 14.40 kernels/hour
Run 2 (-j 6):         255.88 seconds / 14.07 kernels/hour
\f[R]
.fi
.PP
The run with 6 jobs was slower here.
Trying a setting like that by default looks like a waste of time on this
machine, but other machines deliver the best result when they are
oversubscribed a little.
That\[aq]s for example the case on this 6 core/12 threads processor,
which achieved its best result with 15 jobs:
.IP
.nf
\f[C]
[cttest\[at]localhost \[ti]]$ bash kcbench -s 5.3 -n 1
Processor:            Intel(R) Core(TM) i7-8700K CPU \[at] 3.70GHz [12 CPUs]
Cpufreq; Memory:      Unknown; 15934 MByte RAM
Linux running:        5.6.0-0.rc2.git0.1.vanilla.knurd.2.fc31.x86_64
Compiler used:        gcc (GCC) 9.2.1 20190827 (Red Hat 9.2.1-1)
Linux compiled:       5.3.0 [/home/cttest/.cache/kcbench/linux-5.3/]
Config; Environment:  defconfig; CCACHE_DISABLE=\[dq]1\[dq]
Build command:        make vmlinux
Run 1 (-j 12):        92.55 seconds / 38.90 kernels/hour
Run 2 (-j 15):        91.91 seconds / 39.17 kernels/hour
Run 3 (-j 6):         113.66 seconds / 31.67 kernels/hour
Run 4 (-j 9):         101.32 seconds / 35.53 kernels/hour
\f[R]
.fi
.PP
You\[aq]ll notice attempts that tried to utilize only the real cores (-j
6) and oversubscribe them a little (-j 9), which look liked a waste of
time.
But on some machines with SMT capable processors those will deliver the
best results, like on this AMD Threadripper processor with 64 core/128
threads:
.IP
.nf
\f[C]
$ kcbench
[cttest\[at]localhost \[ti]]$ bash kcbench -s 5.3 -n 1
Processor:            AMD Ryzen Threadripper 3990X 64-Core Processor [128 CPUs]
Cpufreq; Memory:      Unknown; 15934 MByte RAM
Linux running:        5.6.0-0.rc2.git0.1.vanilla.knurd.2.fc31.x86_64
Compiler used:        gcc (GCC) 9.2.1 20190827 (Red Hat 9.2.1-1)
Linux compiled:       5.3.0 [/home/cttest/.cache/kcbench/linux-5.3/]
Config; Environment:  defconfig; CCACHE_DISABLE=\[dq]1\[dq]
Build command:        make vmlinux
Run 1 (-j 128):       26.16 seconds / 137.61 kernels/hour
Run 2 (-j 136):       26.19 seconds / 137.46 kernels/hour
Run 3 (-j 64):        21.45 seconds / 167.83 kernels/hour
Run 4 (-j 72):        22.68 seconds / 158.73 kernels/hour
\f[R]
.fi
.PP
This is even more visible when compiling an allmodconfig configuration:
.IP
.nf
\f[C]
[cttest\[at]localhost \[ti]]$ bash kcbench -s 5.3 -n 1 -m
Processor:            AMD Ryzen Threadripper 3990X 64-Core Processor [128 CPUs]
Cpufreq; Memory:      Unknown; 63736 MByte RAM
Linux running:        5.6.0-0.rc2.git0.1.vanilla.knurd.2.fc31.x86_64
Compiler used:        gcc (GCC) 9.2.1 20190827 (Red Hat 9.2.1-1)
Linux compiled:       5.3.0 [/home/cttest/.cache/kcbench/linux-5.3/]
Config; Environment:  defconfig; CCACHE_DISABLE=\[dq]1\[dq]
Build command:        make vmlinux
Run 1 (-j 128):       260.43 seconds / 13.82 kernels/hour
Run 2 (-j 136):       262.67 seconds / 13.71 kernels/hour
Run 3 (-j 64):        215.54 seconds / 16.70 kernels/hour
Run 4 (-j 72):        215.97 seconds / 16.67 kernels/hour
\f[R]
.fi
.PP
This can happen if the SMT implementation is bad or something else
(memory, storage, ...) becomes a bottleneck.
A few tests on above machine indicated the memory interface was the
limiting factor.
A AMD Epyc from the same processor generation did not show this effect
and delivered its best results when the number of jobs matched the
number of CPUs:
.IP
.nf
\f[C]
[cttest\[at]localhost \[ti]]$ bash kcbench -s 5.3 -n 1 -m
Processor:            AMD EPYC 7742 64-Core Processor [256 CPUs]
Cpufreq; Memory:      Unknown; 63736 MByte RAM
Linux running:        5.6.0-0.rc2.git0.1.vanilla.knurd.2.fc31.x86_64
Compiler used:        gcc (GCC) 9.2.1 20190827 (Red Hat 9.2.1-1)
Linux compiled:       5.3.0 [/home/cttest/.cache/kcbench/linux-5.3/]
Config; Environment:  defconfig; CCACHE_DISABLE=\[dq]1\[dq]
Build command:        make vmlinux
Run 1 (-j 256):       128.24 seconds / 28.07 kernels/hour
Run 2 (-j 268):       128.87 seconds / 27.94 kernels/hour
Run 3 (-j 128):       141.83 seconds / 25.38 kernels/hour
Run 4 (-j 140):       137.46 seconds / 26.19 kernels/hour
\f[R]
.fi
.PP
This table will tell you now many jobs kcbench will use by default:
.IP
.nf
\f[C]
 #                             Cores: Default # of jobs
 #                             1 CPU: 1 2
 #           2 CPUs (    no SMT    ): 2 3
 #           2 CPUs (2 threads/core): 2 3 1
 #           4 CPUs (    no SMT    ): 4 6
 #           4 CPUs (2 threads/core): 4 6 2
 #           6 CPUs (    no SMT    ): 6 9
 #           6 CPUs (2 threads/core): 6 9 3
 #           8 CPUs (    no SMT    ): 8 11
 #           8 CPUs (2 threads/core): 8 11 4 6
 #          12 CPUs (    no SMT    ): 12 16
 #          12 CPUs (2 threads/core): 12 16 6 9
 #          16 CPUs (    no SMT    ): 16 20
 #          16 CPUs (2 threads/core): 16 20 8 11
 #          20 CPUs (    no SMT    ): 20 25
 #          20 CPUs (2 threads/core): 20 25 10 14
 #          24 CPUs (    no SMT    ): 24 29
 #          24 CPUs (2 threads/core): 24 29 12 16
 #          28 CPUs (    no SMT    ): 28 34
 #          28 CPUs (2 threads/core): 28 34 14 18
 #          32 CPUs (    no SMT    ): 32 38
 #          32 CPUs (2 threads/core): 32 38 16 20
 #          32 CPUs (4 threads/core): 32 38 8 11
 #          48 CPUs (    no SMT    ): 48 55
 #          48 CPUs (2 threads/core): 48 55 24 29
 #          48 CPUs (4 threads/core): 48 55 12 16
 #          64 CPUs (    no SMT    ): 64 72
 #          64 CPUs (2 threads/core): 64 72 32 38
 #          64 CPUs (4 threads/core): 64 72 16 20
 #         128 CPUs (    no SMT    ): 128 140
 #         128 CPUs (2 threads/core): 128 140 64 72
 #         128 CPUs (4 threads/core): 128 140 32 38
 #         256 CPUs (    no SMT    ): 256 272
 #         256 CPUs (2 threads/core): 256 272 128 140
 #         256 CPUs (4 threads/core): 256 272 64 72
\f[R]
.fi
.SH ON FAILED RUNS DUE TO COMPILATION ERRORS
.PP
The compilation is unlikely to fail, as long as you are using a settled
GCC version to natively compile the source of a current Linux kernel for
popular architectures like ARM, ARM64/Aarch64, or x86_64.
For other cases there is a bigger risk that compilation will fail due to
factors outside of what kcbench/kcbenchrate control.
They nevertheless try to catch a few common problems and warn, but they
can not catch them all, as there are to many factors involved:
.IP \[bu] 2
Brand new compiler generations are sometimes stricter than their
predecessors and thus might fail to compile even the latest Linux kernel
version.
You might need to use a pre-release version of the next Linux kernel
release to make it work or simply need to wait until the compiler or
kernel developers solve the problem.
.IP \[bu] 2
Distributions enable different compiler features that might have an
impact on the kernel compilation.
For example gcc9 was capable of compiling Linux 4.19 on many
distributions, but started to fail on Ubuntu 19.10 due to a feature that
got enabled in its GCC.
Try compiling a newer Linux kernel version in this case.
.IP \[bu] 2
Cross compilation increases the risk of running into compile problems in
general, as there are many compilers and architectures our there.
That for example is why compiling the Linux kernel for an unpopular
architecture is more likely to fail due to bugs in the compiler or the
Linux kernel sources that nobody had noticed before when the compiler or
kernel was released.
This is even more likely to happen if you start kcbench/kcbenchrate with
\[aq]-m/--allmodconfig\[aq] to build a more complex kernel.
.SH HINTS
.PP
Running benchmarks is very tricky.
Here are a few of the aspects you should keep mind when doing so:
.IP \[bu] 2
Do not compare results from two different archs (like ARM64 and x86_64);
kcbench/kcbenchrate compile different code in that case, as they will
compile a native kernel on each of those archs.
This can be avoided by cross compiling for a third arch that is not
related to any of the archs compared (say RISC-V when comparing ARM64
and x86_64).
.IP \[bu] 2
Unless you want to bench compilers do not compare results from different
compiler generations, as they will apply different optimizations
techniques.
For example to not compare results from GCC7 and GCC9, as the later
optimizes harder and thus will take more time generating the code.
That\[aq]s also why the Linux version compiled by default depends on the
machine\[aq]s compiler: you sometimes can\[aq]t compile older kernels
with the latest compilers anyway, as new compiler generations often
uncover bugs in the Linux kernel source that need get fixed for
compiling to succeed.
For example, when GCC10 was close to release it was incapable of compile
the then latest Linux version 5.5 in an allmodconfig configuration due
to a bug in the Linux kernel sources.
.IP \[bu] 2
Compiling a Linux kernel scales very well and thus can utilize
processors quite well.
But be aware that some parts of the Linux compile process will only use
one thread (and thus one CPU core), for example when linking vmlinuz;
the other cores will idle meanwhile.
The effect on the result will grow with the number of CPU cores.
.PP
If you want to work against that consider using \[aq]-m\[aq] to build an
allmodconfig configuration with modules; comping a newer, more complex
Linux kernel version can also help.
But the best way to avoid this effect is by running kcbenchrate.
.IP \[bu] 2
kcbench/kcbenchrate by default set CCACHE_DISABLE=1 when calling
\[aq]make\[aq] to avoid interference from ccache.
.SH EXAMPLES
.TP
To let kcbench decide everything automatically simply run:
$ kcbench
.PP
On a four core processor without SMT kcbench by default will compile 2
kernels with 4 jobs and 2 with 6 jobs.
You can specify a setting like this manually: .
.PP
: $ kcbench -s 5.4 --iterations 3 --jobs 2 --jobs 4
.PP
This will compile Linux 5.4 first 3 times with 2 jobs and then as often
with 4 jobs.
.SH RESULTS
.PP
By default, the lines you are looking for look like this:
.IP
.nf
\f[C]
Run 1 (-j 4): 230.30 sec / 15.63 kernels/hour [P:389%, 24 maj. pagefaults]
\f[R]
.fi
.PP
Here it took 230.30 seconds to compile the Linux kernel image.
With a speed like this the machine can compile 15.63 kernels per hour
(60*60/230.30).
The results from this 4 core machine also show the CPU usage (P) was 389
percent; 24 major page faults occurred during this run \[en] this number
should be small, as processing them takes some time and thus slows down
the build.
This information is omitted, if less than 20 major page faults happen.
For details how the CPU usage is calculated and major page faults are
detected see the man page for GNU \[aq]time\[aq], which
kcbench/kcbenchrate rely on for their measurements.
.PP
When running with \[dq]-d|--detailedresults\[dq] you\[aq]ll get more
detailed result:
.IP
.nf
\f[C]
Run 1 (-j 4): 230.30 sec / 15.63 kernels/hour [P:389%]
Elapsed Time(E): 2:30.10 (150.10 seconds)
Kernel time (S): 36.38 seconds
User time (U): 259.51 seconds
CPU usage (P): 197%
Major page faults (F): 0
Minor page faults (R): 9441809
Context switches involuntarily (c): 69031
Context switches voluntarily (w): 46955
\f[R]
.fi
.SH MISSING FEATURES
.IP \[bu] 2
some math to detect the fastest setting and do one more run with it
before sanity checking the result and printing the best one, including
standard deviation.
.SH SEE ALSO
.PP
\f[B]kcbenchrate(1)\f[R], \f[B]time(1)\f[R]
.SH AUTHOR
.PP
Thorsten Leemhuis <linux [AT] leemhuis [DOT] info>
